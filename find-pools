#!/usr/bin/python3

# This script generates the pools.json data used by the block explorer to map blocks to pools.  It
# needs to be run regularly, e.g. every 30 seconds, to keep the data updates.  It generates two
# files: pools.json, which contains pool information including blocks, and pools-noblocks.json
# containing everything except blocks.  The latter is used by https://pools.graft.observer/ to
# generate an (updating) list of known pools (excluding the block hashes cuts down on the file size
# significantly).

import threading
import time
import sys
import requests
import requests.exceptions
import json.decoder
import random
import os
from collections import deque

results = { 'pools': [], 'updated': time.time(), 'height': 0, 'difficulty': 0 }

class FetchThreads():
    _lock = threading.Lock()
    _done = False
    _tasks = deque()
    _idling = 0
    _threads = []
    _max_threads = 8


    def __init__(self, max_threads = 8):
        """Constructs a new FetchThreads object, taking, optionally, the maximum number of threads
        that will be created to handle the thread jobs."""
        self._max_threads = max_threads


    def _thread_loop(self):
        global results
        idle = False
        lock = self._lock
        lock.acquire()
        while self._tasks or not self._done:
            if self._tasks:
                if idle:
                    idle = False
                    self._idling -= 1
                (name, task, fields, args, kwargs) = self._tasks.popleft()
                lock.release()
                try:
                    res = task(*args, **kwargs)
                    lock.acquire()
                    results['pools'].append({ 'name': name, **fields, 'result': res })
                    lock.release()
                except Exception as e:
                    lock.acquire()
                    err = { 'name': name, **fields }
                    if isinstance(e, requests.exceptions.RequestException):
                        err['error'] = "API request failed: " + str(e)
                    elif isinstance(e, json.decoder.JSONDecodeError):
                        err['error'] = "API returned invalid JSON data"
                    elif isinstance(e, KeyError):
                        err['error'] = "API didn't include expected key '" + e.args[0] + "'"
                    else:
                        err['error'] = "An exception occured while fetching pool data: " + str(e)
                    results['pools'].append(err)
                    lock.release()
            else:
                if not idle:
                    idle = True
                    self._idling += 1
                lock.release()
                time.sleep(.01)

            lock.acquire()
        lock.release()


    def enqueue(self, name, call, fields, *args, **kwargs):
        """Enqueues a task with name `name` into the thread queue.  If there are no idle threads and
        the number of created threads is less than the maximum number of threads, a new thread will
        be created to handle the request immediately; otherwise the task will be added to the queue
        of tasks to be processed when a thread becomes free.  When processing the task, `call` is
        invoked as `call(*args, **kwargs)` with any arguments forwarded to the task when invoked."""
        self._lock.acquire()
        if self._idling == 0 and len(self._threads) < self._max_threads:
            # No current threads are idling, and we haven't hit the max, so spool up a new thread to
            # handle this new task:
            t = threading.Thread(target=self._thread_loop)
            t.start()
            self._threads.append(t)
        self._tasks.append([name, call, fields, args, kwargs])
        self._lock.release()


    def finish(self):
        """Waits for all threads to finish any current tasks and rejoins them."""
        self._lock.acquire()
        self._done = True
        self._lock.release()
        for t in self._threads:
            t.join()


def req_get(*args, **kwargs):
    r = requests.get(*args, timeout=3, **kwargs)
    r.raise_for_status()
    return r


def fetch_ncp(p):
    result = { 'blocks': {} }
    data = req_get(p['api']).json()
    blocks = data['pool']['blocks']
    for i in range(0, len(blocks), 2):
        blkid, height = blocks[i].split(':', 1)[0], blocks[i + 1]
        result['blocks'][height] = blkid

    result['fee'] = data['config']['fee']
    result['payments'] = data['pool']['totalPayments']
    base = float(data['config']['coinUnits' if 'coinUnits' in data['config'] else 'denominationUnit'])
    if p['name'] == 'futurepools.net' and base == 100:
        base = 10000000000.
    result['min'] = data['config'][
            'minPaymentThreshold' if 'minPaymentThreshold' in data['config'] else 'minPaymentThreshold2'] / base
    result['hashrate'] = round(data['pool']['hashrate'])
    result['blocks_found'] = data['pool']['totalBlocks']
    result['miners'] = data['pool']['miners']
    result['miners_paid'] = data['pool']['totalMinersPaid']
    result['payments'] = data['pool']['totalPayments']
    result['height'] = int(data['network']['height'])
    result['difficulty'] = int(data['network']['difficulty'])
    result['effort'] = data['pool']['roundHashes'] / float(result['difficulty']) * 100

    return result

def fetch_nodejs(p):
    result = { 'blocks': {} }
    data = req_get(p['api_blocks']).json()
    for i in data:
        result['blocks'][i['height']] = i['hash']

    pool = req_get(p['api_stats']).json()['pool_statistics']
    network = req_get(p['api_network']).json()
    config = req_get(p['api_config']).json()

    result['hashrate'] = pool['hashRate']
    result['fee'] = config['pplns_fee']
    result['payments'] = pool['totalPayments']
    result['min'] = config['min_wallet_payout'] / 10000000000.
    result['blocks_found'] = pool['totalBlocksFound']
    result['miners'] = pool['miners']
    result['miners_paid'] = pool['totalMinersPaid']
    result['payments'] = pool['totalPayments']

    result['height'] = int(network['height'])
    result['difficulty'] = int(network['difficulty'])
    result['effort'] = pool['roundHashes'] / float(result['difficulty']) * 100

    return result

def fetch_moneroocean(p):
    result = { 'blocks': {} }
    data = req_get(p['api_blocks']).json()
    for i in data:
        result['blocks'][i['height']] = i['hash']

    data = req_get(p['api_stats']).json()
    result['blocks_found'] = data['pool_statistics']['altBlocksFound']['18981']
    graft = data['pool_statistics']['activePort'] == 18981
    result['hashrate'] = data['pool_statistics']['hashRate'] if graft else 0
    result['miners'] = data['pool_statistics']['miners'] if graft else 0

    return result


known_pools = [
    # Name, URL, URL to blocks page, API fetch function, API fetch function args (typically URL(s))
]

def add_ncp(name, url, location, api, blocks_url=None):
    known_pools.append({
        'fetch': fetch_ncp,
        'name': name,
        'location': location,
        'url': url,
        'blocks_url': blocks_url if blocks_url is not None else url + '#pool_blocks',
        'api': api + '/stats',
    })

def add_nodejs(name, url, location, api, blocks_url=None, blocks=25):
    api_blocks = '{}/pool/blocks?page=0&limit={}'.format(api, blocks)
    api_stats = api + '/pool/stats'
    api_network = api + '/network/stats'
    api_config = api + '/config'
    if blocks_url is None:
        blocks_url = url + '#/blocks'

    known_pools.append({
        'fetch': fetch_nodejs,
        'name': name,
        'location': location,
        'url': url,
        'blocks_url': blocks_url if blocks_url is not None else url + '#/blocks',
        'api_blocks': api_blocks,
        'api_stats': api_stats,
        'api_network': api_network,
        'api_config': api_config,
    })


def add_moneroocean():
    known_pools.append({
        'fetch': fetch_moneroocean,
        'name': 'MoneroOcean',
        'location': '(not a dedicated graft pool)',
        'url': 'https://moneroocean.stream/',
        'blocks_url': 'https://moneroocean.stream/#/blocks',
        'api_blocks': 'https://api.moneroocean.stream/pool/coin_altblocks/18981?page=0&limit=50',
        'api_stats': 'https://api.moneroocean.stream/pool/stats',
    })




add_ncp('imaginary', 'https://imaginary.stream/', 'Canada, Montreal', 'https://imaginary.stream/api')
add_ncp('spacepools', 'https://graft.spacepools.org/', 'EU, Germany', 'https://graft.spacepools.org/api')
add_ncp('CryptoKnight', 'https://cryptoknight.cc/graft/', 'EU, Germany', 'https://cryptoknight.cc/rpc/graft')
add_nodejs('Hash Vault', 'https://graft.hashvault.pro/', '', 'https://graft.hashvault.pro/api',
        blocks_url='https://graft.hashvault.pro/en/#!/blocks', blocks=30)
add_ncp('Dev pool', 'http://grftpool.com/', 'EU, Germany', 'http://mining.grftpool.com:8117')
add_ncp('graftmine.net', 'https://www.graftmine.net/', 'US, Virginia', 'https://www.graftmine.net/api')
add_nodejs('graftpool.net', 'http://graftpool.net/', 'EU, France', 'http://graftpool.net/api', blocks=15)
add_nodejs('graft.pw', 'https://www.graft.pw/', 'EU-US-Asia', 'https://api.graft.pw', blocks=15)
add_ncp('easyhash.io', 'https://easyhash.io/graft/', '', 'https://api-grft.easyhash.io',
        blocks_url='https://easyhash.io/graft/blocks'),
add_ncp('thorshammer', 'http://graft.thorshammer.cc/', 'Australia', 'http://graft.thorshammer.cc:8117')
add_ncp('miner.rocks', 'https://graft.miner.rocks/', 'US, New Jersey', 'https://graft.miner.rocks/api')
add_nodejs('cryptopool.space', 'https://grft.cryptopool.space/en/', 'Ru-EU-Asia-US', 'https://grft.cryptopool.space/api', blocks=15)
add_ncp('BlockHashMining', 'https://graft.blockhashmining.com/', 'US, New York', 'https://graft.blockhashmining.com/api')
add_nodejs('cryptocrush', 'http://graft.cryptocrush.cc/', 'US, West', 'http://mine.graft.cryptocrush.cc:8001', blocks=15)
add_nodejs('poolshash.net', 'https://graft.poolshash.net/', 'Asia, Singapore', 'https://api.graft.hashnetwork.net', blocks=15)
add_ncp('MineUnlimited', 'http://graft.mineunlimited.com/', 'Asia, Singapore', 'http://asia.graft.mineunlimited.com/api')
add_ncp('graftpool.fr', 'http://graftpool.fr/', 'EU, France', 'http://graftpool.fr:8117')
add_nodejs('graftpool.network', 'https://graftpool.network/', 'EU, Netherlands', 'https://graftpool.network/api', blocks=15)
add_ncp('dark-mine.su', 'https://graft.dark-mine.su/', 'Russia; Germany', 'https://graft.dark-mine.su:15004')
add_ncp('anypool.net', 'https://graft.anypool.net/', 'EU, France', 'https://api.graft.anypool.net')
add_ncp('nocroom', 'http://graft.pool.nocroom.com/', 'US, Los Angeles', 'http://graft.pool.nocroom.com:8117')
add_nodejs('graftpro.com', 'https://graftpro.com/', 'US-EU-AP-Ru', 'https://api.graftpro.com/api', blocks=15)
add_ncp('futurepools.net', 'http://grft.futurepools.net/', 'US, East', 'http://graft-api.futurepools.net:8117')
add_nodejs('supportcryptonight', 'https://graftpool.supportcryptonight.com/', 'EU, France', 'https://graft.supportcryptonight.com/api', blocks=15)
add_nodejs('cryptonightminers', 'https://graft.cryptonightminers.online/', 'AP-US-EU', 'https://graft.cryptonightminers.online/api', blocks=15)
add_nodejs('semiPOOL', 'https://grft.semipool.com/', 'EU, France', 'https://grft-api.semipool.com', blocks=15)
add_nodejs('herominers', 'https://graft.herominers.com/', 'EU, France', 'https://graft.herominers.com/api', blocks=15)
add_ncp('cryptoblock.uk', 'https://grft.cryptoblock.uk/', 'UK, London', 'https://grft.cryptoblock.uk/api')
add_moneroocean()



threads = FetchThreads()

random.shuffle(known_pools)
for p in known_pools:
    fields = {}
    for x in ('url', 'blocks_url', 'location'):
        if x in p:
            fields[x] = p[x]
    threads.enqueue(p['name'], p['fetch'], fields, p)


def fetch_netinfo(p):
    return req_get(p['url'] + '/api/networkinfo').json()['data']

threads.enqueue('_graft.observer', fetch_netinfo, { 'explorer': True }, { 'url': 'https://graft.observer' });
threads.enqueue('_blockexplorer.graft.network', fetch_netinfo, { 'explorer': True }, { 'url': 'https://blockexplorer.graft.network' });

threads.finish()

heights, popular_height, popular_height_count, height_diff = {}, {}, 0, {}
expl_height, expl_diff = 0, 0
for p in results['pools']:
    if 'result' in p:
        result = p['result']
        if 'explorer' in p:
            if result['height'] > expl_height:
                expl_height = result['height'];
                expl_diff = result['difficulty'];
        elif 'height' in result:
            height = result['height']
            if height not in heights:
                heights[height] = 1
            else:
                heights[height] += 1
            if heights[height] > popular_height_count:
                popular_height = height
                popular_height_count = heights[height]
            height_diff[height] = result['difficulty']


# Look for out-of-sync pools; this can happen in two ways: a pool could have desynched (or not
# forked) and stalled, with height too low, or it could have desynched and run away, with height too
# high.  On the other hand, it's normal to see pools 1-2 blocks out of sync (i.e. the API scraping
# could occur around the same time a block is discovered).
#
# So: look for the most common height and then allow anything up to +2; the highest one we find is
# what we use for current network diff/hashrate and is what we use the the '% net' calculation.
# Anything above that highest accepted height, or anything more than 2 under it, gets flagged as a
# problem.

results['hashrate_synched'] = 0
results['hashrate_unsynched'] = 0
for p in results['pools']:
    if 'result' in p and 'height' in p['result']:
        result = p['result']
        h = result['height']
        if results['height'] < h <= popular_height + 3:
            results['height'] = h
            results['difficulty'] = result['difficulty']

# The block explorers report the diff of the current block, while pools report the diff of the last
# block found, so as long as it isn't behind the explorer is a better source of data:
print("{} >=? {}".format(expl_height, results['height']))
if expl_height >= results['height']:
    results['height'] = expl_height
    results['difficulty'] = expl_diff

results['diff_target_hashrate'] = results['difficulty'] / 120.0

for p in results['pools']:
    if 'result' in p and 'explorer' not in p:
        if 'height' in p['result'] and not 0 <= results['height'] - p['result']['height'] <= 3:
            p['desync'] = True
            results['hashrate_unsynched'] += p['result']['hashrate']
        else:
            results['hashrate_synched'] += p['result']['hashrate']


with open('pools.json.tmp', 'w') as out:
    out.write(json.dumps(results))
os.replace('pools.json.tmp', 'pools.json')

for p in results['pools']:
    if 'result' in p and 'blocks' in p['result']:
        del p['result']['blocks']
with open('pools-noblocks.json.tmp', 'w') as out:
    out.write(json.dumps(results))
os.replace('pools-noblocks.json.tmp', 'pools-noblocks.json')
