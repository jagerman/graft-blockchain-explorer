#!/usr/bin/python3

# This script generates the pools.json data used by the block explorer to map blocks to pools.  It
# needs to be run regularly, e.g. every 30 seconds, to keep the data updates.  It generates two
# files: pools.json, which contains pool information including blocks, and pools-noblocks.json
# containing everything except blocks.  The latter is used by https://pools.graft.observer/ to
# generate an (updating) list of known pools (excluding the block hashes cuts down on the file size
# significantly).

# It relies on a postgresql database (connection info below) with this structure:
#
# (FIXME)

import threading
import time
import sys
import requests, requests.exceptions
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.poolmanager import PoolManager
import json.decoder
import random
import os
import psycopg2, psycopg2.extras
from collections import deque

pgsql = psycopg2.connect(dbname='jgr',
        cursor_factory=psycopg2.extras.DictCursor)

results = { 'pools': [], 'updated': time.time(), 'height': 0, 'difficulty': 0 }

class FetchThreads():
    _lock = threading.Lock()
    _done = False
    _tasks = deque()
    _idling = 0
    _threads = []
    _max_threads = 8


    def __init__(self, max_threads = 8):
        """Constructs a new FetchThreads object, taking, optionally, the maximum number of threads
        that will be created to handle the thread jobs."""
        self._max_threads = max_threads


    def _thread_loop(self):
        global results
        idle = False
        lock = self._lock
        lock.acquire()
        while self._tasks or not self._done:
            if self._tasks:
                if idle:
                    idle = False
                    self._idling -= 1
                (name, task, fields, args, kwargs) = self._tasks.popleft()
                lock.release()
                try:
                    res = task(*args, **kwargs)
                    lock.acquire()
                    results['pools'].append({ 'name': name, **fields, 'result': res })
                    lock.release()
                except Exception as e:
                    lock.acquire()
                    err = { 'name': name, **fields }
                    if isinstance(e, requests.exceptions.RequestException):
                        err['error'] = "API request failed: " + str(e)
                    elif isinstance(e, json.decoder.JSONDecodeError):
                        err['error'] = "API returned invalid JSON data"
                    elif isinstance(e, KeyError):
                        err['error'] = "API didn't include expected key '" + e.args[0] + "'"
                    else:
                        err['error'] = "An exception occured while fetching pool data: " + str(e)
                    results['pools'].append(err)
                    lock.release()
            else:
                if not idle:
                    idle = True
                    self._idling += 1
                lock.release()
                time.sleep(.01)

            lock.acquire()
        lock.release()


    def enqueue(self, name, call, fields, *args, **kwargs):
        """Enqueues a task with name `name` into the thread queue.  If there are no idle threads and
        the number of created threads is less than the maximum number of threads, a new thread will
        be created to handle the request immediately; otherwise the task will be added to the queue
        of tasks to be processed when a thread becomes free.  When processing the task, `call` is
        invoked as `call(*args, **kwargs)` with any arguments forwarded to the task when invoked."""
        self._lock.acquire()
        if self._idling == 0 and len(self._threads) < self._max_threads:
            # No current threads are idling, and we haven't hit the max, so spool up a new thread to
            # handle this new task:
            t = threading.Thread(target=self._thread_loop)
            t.start()
            self._threads.append(t)
        self._tasks.append([name, call, fields, args, kwargs])
        self._lock.release()


    def finish(self):
        """Waits for all threads to finish any current tasks and rejoins them."""
        self._lock.acquire()
        self._done = True
        self._lock.release()
        for t in self._threads:
            t.join()


def req_get(*args, **kwargs):
    r = requests.get(*args, timeout=3, **kwargs)
    r.raise_for_status()
    return r


def req_post(*args, **kwargs):
    r = requests.post(*args, timeout=3, **kwargs)
    r.raise_for_status()
    return r


def fetch_ncp(p):
    result = { 'blocks': {} }
    data = req_get(p['api_url'] + '/stats').json()
    blocks = data['pool']['blocks']
    for i in range(0, len(blocks), 2):
        blkid, height = blocks[i].split(':', 1)[0], blocks[i + 1]
        result['blocks'][height] = blkid

    result['fee'] = data['config']['fee']
    result['payments'] = data['pool']['totalPayments']
    base = float(data['config']['coinUnits' if 'coinUnits' in data['config'] else 'denominationUnit'])
    if p['name'] == 'futurepools.net' and base == 100:
        base = 10000000000.
    result['threshold'] = data['config'][
            'minPaymentThreshold' if 'minPaymentThreshold' in data['config'] else 'minPaymentThreshold2'] / base
    result['hashrate'] = round(data['pool']['hashrate'])
    result['blocks_found'] = data['pool']['totalBlocks']
    result['miners'] = data['pool']['miners']
    result['miners_paid'] = data['pool']['totalMinersPaid']
    result['payments'] = data['pool']['totalPayments']
    result['height'] = int(data['network']['height'])
    result['difficulty'] = int(data['network']['difficulty'])
    result['effort'] = data['pool']['roundHashes'] / float(result['difficulty']) * 100

    return result

def fetch_nodejs(p):
    result = { 'blocks': {} }

    api = p['api_url']

    data = req_get('{}/pool/blocks?page=0&limit={}'.format(api, 15)).json()
    for i in data:
        result['blocks'][i['height']] = i['hash']

    pool = req_get(api + '/pool/stats').json()['pool_statistics']
    network = req_get(api + '/network/stats').json()
    config = req_get(api + '/config').json()

    result['hashrate'] = pool['hashRate']
    result['fee'] = config['pplns_fee']
    result['payments'] = pool['totalPayments']
    result['threshold'] = config['min_wallet_payout'] / 10000000000.
    result['blocks_found'] = pool['totalBlocksFound']
    result['miners'] = pool['miners']
    result['miners_paid'] = pool['totalMinersPaid']
    result['payments'] = pool['totalPayments']

    result['height'] = int(network['height'])
    result['difficulty'] = int(network['difficulty'])
    result['effort'] = pool['roundHashes'] / float(result['difficulty']) * 100

    return result

def fetch_moneroocean(p):
    result = { 'blocks': {} }

    api = p['api_url']
    data = req_get(api + '/coin_altblocks/18981?page=0&limit=15').json()
    for i in data:
        result['blocks'][i['height']] = i['hash']

    data = req_get(api + '/stats').json()
    result['blocks_found'] = data['pool_statistics']['altBlocksFound']['18981']
    graft = data['pool_statistics']['activePort'] == 18981
    result['hashrate'] = data['pool_statistics']['hashRate'] if graft else 0
    result['miners'] = data['pool_statistics']['miners'] if graft else 0

    return result


known_pools = []

cur = pgsql.cursor()
cur.execute("SELECT * FROM pools WHERE enabled")
pool_ids = {}
for row in cur:
    pool_ids[row['name']] = row['id']
    data = dict(row)
    if row['api'] == 'ncp':
        data['fetch'] = fetch_ncp
    elif row['api'] == 'nodejs':
        data['fetch'] = fetch_nodejs
    elif row['api'] == 'moneroocean':
        data['fetch'] = fetch_moneroocean
    else:
        raise RuntimeError("Unkown pool API: {}".format(row['api']))

    known_pools.append(data)

node_info = req_post('http://127.0.0.1:18981/json_rpc', json={ "jsonrpc": "2.0", "id": "0", "method": "get_info" }).json()['result']

threads = FetchThreads()

random.shuffle(known_pools)
for p in known_pools:
    fields = {}
    for x in ('url', 'blocks_url', 'location'):
        if x in p:
            fields[x] = p[x]
    threads.enqueue(p['name'], p['fetch'], fields, p)

threads.finish()

cur.execute("DELETE FROM pool_fetches WHERE time < NOW() + '7 days 1 hour ago'")
cur.execute("INSERT INTO pool_fetches (height, difficulty) VALUES (%s, %s) RETURNING id",
    (node_info['height'], node_info['difficulty']))
fetch_id = cur.fetchone()[0]

for p in results['pools']:
    poolid = pool_ids[p['name']]
    if 'result' in p:
        r = p['result']
        if 'error' in r:
            cur.execute('INSERT INTO pool_stats (pool_fetch, pool, error) VALUES (%s, %s, %s)', (fetch_id, poolid, r['error']))
        else:
            for x in ('height', 'blocks_found', 'hashrate', 'effort', 'miners', 'miners_paid', 'payments', 'fee', 'threshold'):
                if x not in r:
                    r[x] = None
            cur.execute('INSERT INTO pool_stats (pool_fetch, pool, height, blocks_found, hashrate, effort, miners, miners_paid, payments, fee, threshold) ' +
                           'VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)',
                           (fetch_id, poolid, r['height'], r['blocks_found'], r['hashrate'], r['effort'], r['miners'], r['miners_paid'], r['payments'], r['fee'], r['threshold'])
            )

            for height, blkid in r['blocks'].items():
                cur.execute('INSERT INTO pool_blocks (pool, height, hash) VALUES (%s, %s, %s) ON CONFLICT DO NOTHING',
                        (poolid, height, blkid))
    elif 'error' in p:
        cur.execute('INSERT INTO pool_stats (pool_fetch, pool, error) VALUES (%s, %s, %s)', (fetch_id, poolid, p['error']))
del cur
pgsql.commit()

pgsql.cursor().execute('REFRESH MATERIALIZED VIEW pool_agg_stats')
pgsql.cursor().execute('REFRESH MATERIALIZED VIEW pool_hashrate_chart')
pgsql.commit()
